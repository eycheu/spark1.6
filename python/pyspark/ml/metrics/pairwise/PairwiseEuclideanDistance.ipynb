{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sc: <pyspark.context.SparkContext object at 0x7f4523710908>\n",
      "Created sqlContext: <pyspark.sql.context.SQLContext object at 0x7f45237108d0>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if isinstance(sc, pyspark.SparkContext):\n",
    "        print(\"sc:\", sc)\n",
    "except Exception as ex:\n",
    "    sc = pyspark.SparkContext()\n",
    "    print(\"Created sc:\", sc)\n",
    "    \n",
    "try:\n",
    "    if isinstance(sqlContext, pyspark.SQLContext):\n",
    "        print(\"sqlContext:\", sqlContext)\n",
    "except Exception as ex:\n",
    "    sqlContext = pyspark.SQLContext(sc)\n",
    "    print(\"Created sqlContext:\", sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.ml\n",
    "import pyspark.mllib.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|  feature|\n",
      "+---------+\n",
      "|[0.0,1.0]|\n",
      "|[1.0,1.0]|\n",
      "|[2.0,3.0]|\n",
      "|[8.0,9.0]|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(feature=DenseVector([0.0, 1.0])),\n",
       " Row(feature=DenseVector([1.0, 1.0])),\n",
       " Row(feature=DenseVector([2.0, 3.0])),\n",
       " Row(feature=DenseVector([8.0, 9.0]))]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data = [(pyspark.mllib.linalg.Vectors.dense([0.0, 1.0]),), \n",
    "            (pyspark.mllib.linalg.Vectors.dense([1.0, 1.0]),), \n",
    "            (pyspark.mllib.linalg.Vectors.dense([2.0, 3.0]),), \n",
    "            (pyspark.mllib.linalg.Vectors.dense([8.0, 9.0]),)]\n",
    "df_data = sqlContext.createDataFrame(rdd_data, [\"feature\"])\n",
    "df_data.show()\n",
    "df_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([0.0, 1.0]),\n",
       " DenseVector([1.0, 1.0]),\n",
       " DenseVector([2.0, 3.0]),\n",
       " DenseVector([8.0, 9.0])]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df_data.select(\"feature\").map(lambda r: r.feature)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, DenseVector([0.0, 1.0])),\n",
       " (1, DenseVector([1.0, 1.0])),\n",
       " (2, DenseVector([2.0, 3.0])),\n",
       " (3, DenseVector([8.0, 9.0]))]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.zipWithIndex()\n",
    "rdd = rdd.flatMap(lambda r: [(r[1],r[0])]*1)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|  feature|\n",
      "+---+---------+\n",
      "|  0|[0.0,1.0]|\n",
      "|  1|[1.0,1.0]|\n",
      "|  2|[2.0,3.0]|\n",
      "|  3|[8.0,9.0]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(rdd, [\"id\", \"feature\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----+---------+\n",
      "|x_id|x_feature|y_id|y_feature|\n",
      "+----+---------+----+---------+\n",
      "|   0|[0.0,1.0]|   0|[0.0,1.0]|\n",
      "|   0|[0.0,1.0]|   1|[1.0,1.0]|\n",
      "|   0|[0.0,1.0]|   2|[2.0,3.0]|\n",
      "|   0|[0.0,1.0]|   3|[8.0,9.0]|\n",
      "|   1|[1.0,1.0]|   0|[0.0,1.0]|\n",
      "|   1|[1.0,1.0]|   1|[1.0,1.0]|\n",
      "|   1|[1.0,1.0]|   2|[2.0,3.0]|\n",
      "|   1|[1.0,1.0]|   3|[8.0,9.0]|\n",
      "|   2|[2.0,3.0]|   0|[0.0,1.0]|\n",
      "|   2|[2.0,3.0]|   1|[1.0,1.0]|\n",
      "|   2|[2.0,3.0]|   2|[2.0,3.0]|\n",
      "|   2|[2.0,3.0]|   3|[8.0,9.0]|\n",
      "|   3|[8.0,9.0]|   0|[0.0,1.0]|\n",
      "|   3|[8.0,9.0]|   1|[1.0,1.0]|\n",
      "|   3|[8.0,9.0]|   2|[2.0,3.0]|\n",
      "|   3|[8.0,9.0]|   3|[8.0,9.0]|\n",
      "+----+---------+----+---------+\n",
      "\n",
      "Row(x_id=0, x_feature=DenseVector([0.0, 1.0]), y_id=0, y_feature=DenseVector([0.0, 1.0]))\n",
      "Row(x_id=0, x_feature=DenseVector([0.0, 1.0]), y_id=1, y_feature=DenseVector([1.0, 1.0]))\n",
      "Row(x_id=0, x_feature=DenseVector([0.0, 1.0]), y_id=2, y_feature=DenseVector([2.0, 3.0]))\n",
      "Row(x_id=0, x_feature=DenseVector([0.0, 1.0]), y_id=3, y_feature=DenseVector([8.0, 9.0]))\n",
      "Row(x_id=1, x_feature=DenseVector([1.0, 1.0]), y_id=0, y_feature=DenseVector([0.0, 1.0]))\n",
      "Row(x_id=1, x_feature=DenseVector([1.0, 1.0]), y_id=1, y_feature=DenseVector([1.0, 1.0]))\n",
      "Row(x_id=1, x_feature=DenseVector([1.0, 1.0]), y_id=2, y_feature=DenseVector([2.0, 3.0]))\n",
      "Row(x_id=1, x_feature=DenseVector([1.0, 1.0]), y_id=3, y_feature=DenseVector([8.0, 9.0]))\n",
      "Row(x_id=2, x_feature=DenseVector([2.0, 3.0]), y_id=0, y_feature=DenseVector([0.0, 1.0]))\n",
      "Row(x_id=2, x_feature=DenseVector([2.0, 3.0]), y_id=1, y_feature=DenseVector([1.0, 1.0]))\n",
      "Row(x_id=2, x_feature=DenseVector([2.0, 3.0]), y_id=2, y_feature=DenseVector([2.0, 3.0]))\n",
      "Row(x_id=2, x_feature=DenseVector([2.0, 3.0]), y_id=3, y_feature=DenseVector([8.0, 9.0]))\n",
      "Row(x_id=3, x_feature=DenseVector([8.0, 9.0]), y_id=0, y_feature=DenseVector([0.0, 1.0]))\n",
      "Row(x_id=3, x_feature=DenseVector([8.0, 9.0]), y_id=1, y_feature=DenseVector([1.0, 1.0]))\n",
      "Row(x_id=3, x_feature=DenseVector([8.0, 9.0]), y_id=2, y_feature=DenseVector([2.0, 3.0]))\n",
      "Row(x_id=3, x_feature=DenseVector([8.0, 9.0]), y_id=3, y_feature=DenseVector([8.0, 9.0]))\n",
      "root\n",
      " |-- x_id: long (nullable = true)\n",
      " |-- x_feature: vector (nullable = true)\n",
      " |-- y_id: long (nullable = true)\n",
      " |-- y_feature: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.join(df).toDF('x_id', 'x_feature', 'y_id', 'y_feature')\n",
    "df2.show()\n",
    "for i in df2.collect():\n",
    "    print(i)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+----+---------+----+\n",
      "|          distance|x_feature|x_id|y_feature|y_id|\n",
      "+------------------+---------+----+---------+----+\n",
      "|               0.0|[0.0,1.0]|   0|[0.0,1.0]|   0|\n",
      "|               1.0|[0.0,1.0]|   0|[1.0,1.0]|   1|\n",
      "|2.8284271247461903|[0.0,1.0]|   0|[2.0,3.0]|   2|\n",
      "|11.313708498984761|[0.0,1.0]|   0|[8.0,9.0]|   3|\n",
      "|               1.0|[1.0,1.0]|   1|[0.0,1.0]|   0|\n",
      "|               0.0|[1.0,1.0]|   1|[1.0,1.0]|   1|\n",
      "|  2.23606797749979|[1.0,1.0]|   1|[2.0,3.0]|   2|\n",
      "| 10.63014581273465|[1.0,1.0]|   1|[8.0,9.0]|   3|\n",
      "|2.8284271247461903|[2.0,3.0]|   2|[0.0,1.0]|   0|\n",
      "|  2.23606797749979|[2.0,3.0]|   2|[1.0,1.0]|   1|\n",
      "|               0.0|[2.0,3.0]|   2|[2.0,3.0]|   2|\n",
      "|  8.48528137423857|[2.0,3.0]|   2|[8.0,9.0]|   3|\n",
      "|11.313708498984761|[8.0,9.0]|   3|[0.0,1.0]|   0|\n",
      "| 10.63014581273465|[8.0,9.0]|   3|[1.0,1.0]|   1|\n",
      "|  8.48528137423857|[8.0,9.0]|   3|[2.0,3.0]|   2|\n",
      "|               0.0|[8.0,9.0]|   3|[8.0,9.0]|   3|\n",
      "+------------------+---------+----+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd2 = df2.map(lambda r: pyspark.sql.Row(x_id=r.x_id, \n",
    "                                         x_feature=r.x_feature,\n",
    "                                         y_id=r.y_id,\n",
    "                                         y_feature=r.y_feature,\n",
    "                                         distance=float(r.x_feature.squared_distance(r.y_feature))**0.5 ))\n",
    "df2 = sqlContext.createDataFrame(rdd2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- distance: double (nullable = true)\n",
      " |-- x_feature: vector (nullable = true)\n",
      " |-- x_id: long (nullable = true)\n",
      " |-- y_feature: vector (nullable = true)\n",
      " |-- y_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|  feature|\n",
      "+---------+\n",
      "|[0.0,1.0]|\n",
      "|[1.0,1.0]|\n",
      "|[2.0,3.0]|\n",
      "|[8.0,9.0]|\n",
      "+---------+\n",
      "\n",
      "inputCol: input column name. (default: feature, current: feature)\n",
      "outputCol: output column name. (default: distance, current: distance)\n",
      "squared: boolean to indicate squared Euclidean distance (default: False, current: True)\n",
      "+----+---------+----+---------+--------+\n",
      "|x_id|x_feature|y_id|y_feature|distance|\n",
      "+----+---------+----+---------+--------+\n",
      "|   0|[0.0,1.0]|   0|[0.0,1.0]|     0.0|\n",
      "|   0|[0.0,1.0]|   1|[1.0,1.0]|     1.0|\n",
      "|   0|[0.0,1.0]|   2|[2.0,3.0]|     8.0|\n",
      "|   0|[0.0,1.0]|   3|[8.0,9.0]|   128.0|\n",
      "|   1|[1.0,1.0]|   0|[0.0,1.0]|     1.0|\n",
      "|   1|[1.0,1.0]|   1|[1.0,1.0]|     0.0|\n",
      "|   1|[1.0,1.0]|   2|[2.0,3.0]|     5.0|\n",
      "|   1|[1.0,1.0]|   3|[8.0,9.0]|   113.0|\n",
      "|   2|[2.0,3.0]|   0|[0.0,1.0]|     8.0|\n",
      "|   2|[2.0,3.0]|   1|[1.0,1.0]|     5.0|\n",
      "|   2|[2.0,3.0]|   2|[2.0,3.0]|     0.0|\n",
      "|   2|[2.0,3.0]|   3|[8.0,9.0]|    72.0|\n",
      "|   3|[8.0,9.0]|   0|[0.0,1.0]|   128.0|\n",
      "|   3|[8.0,9.0]|   1|[1.0,1.0]|   113.0|\n",
      "|   3|[8.0,9.0]|   2|[2.0,3.0]|    72.0|\n",
      "|   3|[8.0,9.0]|   3|[8.0,9.0]|     0.0|\n",
      "+----+---------+----+---------+--------+\n",
      "\n",
      "inputCol: input column name. (default: feature)\n",
      "outputCol: output column name. (default: distance)\n",
      "squared: boolean to indicate squared Euclidean distance (default: False, current: False)\n",
      "+----+---------+----+---------+------------------+\n",
      "|x_id|x_feature|y_id|y_feature|          distance|\n",
      "+----+---------+----+---------+------------------+\n",
      "|   0|[0.0,1.0]|   0|[0.0,1.0]|               0.0|\n",
      "|   0|[0.0,1.0]|   1|[1.0,1.0]|               1.0|\n",
      "|   0|[0.0,1.0]|   2|[2.0,3.0]|2.8284271247461903|\n",
      "|   0|[0.0,1.0]|   3|[8.0,9.0]|11.313708498984761|\n",
      "|   1|[1.0,1.0]|   0|[0.0,1.0]|               1.0|\n",
      "|   1|[1.0,1.0]|   1|[1.0,1.0]|               0.0|\n",
      "|   1|[1.0,1.0]|   2|[2.0,3.0]|  2.23606797749979|\n",
      "|   1|[1.0,1.0]|   3|[8.0,9.0]| 10.63014581273465|\n",
      "|   2|[2.0,3.0]|   0|[0.0,1.0]|2.8284271247461903|\n",
      "|   2|[2.0,3.0]|   1|[1.0,1.0]|  2.23606797749979|\n",
      "|   2|[2.0,3.0]|   2|[2.0,3.0]|               0.0|\n",
      "|   2|[2.0,3.0]|   3|[8.0,9.0]|  8.48528137423857|\n",
      "|   3|[8.0,9.0]|   0|[0.0,1.0]|11.313708498984761|\n",
      "|   3|[8.0,9.0]|   1|[1.0,1.0]| 10.63014581273465|\n",
      "|   3|[8.0,9.0]|   2|[2.0,3.0]|  8.48528137423857|\n",
      "|   3|[8.0,9.0]|   3|[8.0,9.0]|               0.0|\n",
      "+----+---------+----+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pyspark.ml.param\n",
    "from pyspark.ml.param.shared import *\n",
    "\n",
    "@pyspark.mllib.common.inherit_doc\n",
    "class PairwiseEuclideanDistance(pyspark.ml.pipeline.Transformer, HasInputCol, HasOutputCol):\n",
    "    \n",
    "    # a placeholder to make it appear in the generated doc\n",
    "    squared = pyspark.ml.param.Param(Params._dummy(), \"squared\", \n",
    "                                     \"boolean to indicate squared Euclidean distance\")\n",
    "\n",
    "    @pyspark.ml.util.keyword_only\n",
    "    def __init__(self, squared=False, inputCol=\"feature\", outputCol=\"distance\"):\n",
    "        \"\"\"\n",
    "        __init__(self, squared=False, outputCol=None)\n",
    "        \"\"\"\n",
    "        super(PairwiseEuclideanDistance, self).__init__()\n",
    "        self.squared = pyspark.ml.param.Param(self, \"squared\", \n",
    "                                     \"boolean to indicate squared Euclidean distance\")\n",
    "        self._setDefault(squared=False, inputCol=\"feature\", outputCol=\"distance\")\n",
    "        kwargs = self.__init__._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "    \n",
    "    @pyspark.ml.util.keyword_only\n",
    "    def setParams(self, squared=False, inputCol=\"feature\", outputCol=\"distance\"):\n",
    "        \"\"\"\n",
    "        setParams(self, squared=False, outputCol=None)\n",
    "        Sets params for this EuclideanPairwiseDistance.\n",
    "        \"\"\"\n",
    "        kwargs = self.setParams._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def setSquared(self, value):\n",
    "        \"\"\"\n",
    "        Sets the value of :py:attr:`squared`.\n",
    "        \"\"\"\n",
    "        self._paramMap[self.squared] = value\n",
    "        return self\n",
    "\n",
    "    def getSquared(self):\n",
    "        \"\"\"\n",
    "        Gets the value of squared or its default value.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.squared)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"\n",
    "        Transforms the input dataset.\n",
    "        \"\"\"\n",
    "        rdd = dataset.select(self.getInputCol()).map(lambda r: r[0])\n",
    "        rdd = rdd.zipWithIndex()\n",
    "        df = rdd.toDF()\n",
    "        df = df.join(df)\n",
    "        if self.getSquared():\n",
    "            rdd = df.map(lambda r: [r[1], r[0], r[3], r[2], float(r[0].squared_distance(r[2]))])\n",
    "        else:\n",
    "            rdd = df.map(lambda r: [r[1], r[0], r[3], r[2], float(r[0].squared_distance(r[2]))**0.5])\n",
    "        df = rdd.toDF([\"x_id\", \"x_\"+self.getInputCol(), \"y_id\", \"y_\"+self.getInputCol(), self.getOutputCol()])\n",
    "        return df\n",
    "    \n",
    "\n",
    "rdd_data = [(pyspark.mllib.linalg.Vectors.dense([0.0, 1.0]),), \n",
    "            (pyspark.mllib.linalg.Vectors.dense([1.0, 1.0]),), \n",
    "            (pyspark.mllib.linalg.Vectors.dense([2.0, 3.0]),), \n",
    "            (pyspark.mllib.linalg.Vectors.dense([8.0, 9.0]),)]\n",
    "df_data = sqlContext.createDataFrame(rdd_data, [\"feature\"])\n",
    "df_data.show()\n",
    "df_data.collect()\n",
    "\n",
    "ped = PairwiseEuclideanDistance(squared=False, inputCol=\"feature\", outputCol=\"distance\")\n",
    "\n",
    "ped.setSquared(True)\n",
    "ped.getSquared()\n",
    "ped.getInputCol()\n",
    "ped.hasDefault(\"squared\")\n",
    "print(ped.explainParams())\n",
    "ped.transform(df_data).show()\n",
    "ped.setSquared(False)\n",
    "print(a.explainParams())\n",
    "ped.transform(df_data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([0.0, 1.0]),\n",
       " DenseVector([1.0, 1.0]),\n",
       " DenseVector([2.0, 3.0]),\n",
       " DenseVector([8.0, 9.0])]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df_data.select(a.getInputCol()).map(lambda r: r.feature)\n",
    "rdd.collect()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
